# Meeting : 24/10/2023

* Time: 14:00-14:30
* Location: SAWB 331
----------

* Project: Watermarking in Machine-Generated Text
* Student: Samuel Jackson
* Student ID: 2520998J
* Supervisor: Dr. Jake Lever
----------

### Agenda

- Changes to project/impact [10 mins]
- Lack of paraphrasers [5 mins]
- Methods of checking whether a watermarking is *correctly* implemented [5 mins]
  - This is more of a question because I don't really know what to do
- Deciding appropriate graphs / visual displays of topic [5 mins]

### Progress

- Realised that I cannot evaluate every model and should appropriately switch project
- Found some watermarks to implement instead
- Made code adaptable to allow for user-chosen watermark - easy evaluation process
- Decided relevant variables to store for generating graphs
- Started looking into writing methodology
- Organised repository

### Questions
- Different watermarks can have different z-scores, would this affect comparison of z-scores?
- If I am to generate sample documents of watermarked text, would I want to make sure I am watermarking the same sample documents for each watermark?

#### Research Question
- How does a sliding window of watermark detection affect Z-Score average? (High-std signify watermark?)
  - As opposed to detection on an entire essay which may have only used GPT for one paragraph.

### Meeting Minutes

- Notes throughout session

### Summary

- Perhaps email author for code.
- So sentence-based paraphrasing models.
- Is sliding window realistic as this project - no, perhaps another project.
- Keep the same sample documents for each watermark.
- Hard to compare z-scores.
- The z-score might be comparable but not always.
- Perhaps RoC curve is useful.

### Next Meeting Goals

- Things to do for next meeting


