# Meeting : 07/02/2024

* Time: 16:00-16:30
* Location: SAWB 331
----------

* Project: Watermarking in Machine-Generated Text
* Student: Samuel Jackson
* Student ID: 2520998J
* Supervisor: Dr. Jake Lever
----------

### Agenda

- Go over background topics [10 mins]
- Discuss design stage and analysis stage of diss [5 mins]

### Progress

- Filling out background section.
- Motivation section draft complete.
- Beginning anaylsis stage.

### Questions

- Why do we use logits?
- Clarification on Seq2Seq models
- Writing style: Am I aiming for concise descriptions or is providing examples and images a more appropriate style?
- Questions about tokenisation and uniqueness of a tokeniser:
  - Can tokeniser, whilst using the same technique, produce unique tokenisations on a given document?
  - Is there a potential for universal tokenisers?
- Questions about uniqueness of vectorisation:
  - Are word-embeddings generated as part of the pre-training process?
  - Are these word-embeddings unique to a given model as well?

### Meeting Minutes

- Logits are convention in NLP.
- Concise is fine but images are good. Guide the reader.
- Universal tokenisers could exist but not likely.
- Tokenisation training is deterministic, but requires the same dataset for training. 


